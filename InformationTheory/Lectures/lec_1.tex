\chapter{Shannon's Entropy}
\section{Entropy Function}
\begin{definition}
    Entropy function is introduced to determine the \textit{uncertainty of a random variable $X$}. Providing two \textbf{intuitions}
    \begin{enumerate}
        \item The number of bits of information that we don't know about $X$
        \item The number of bits needed to describe $X$
    \end{enumerate}
    \begin{align}
        H(X) &= -\sum_{x} P(X = x) \log{P(X = x)} 
        \\ &= -\mathbb{E}_X \left[ \log{P(X)} \right]
        \\ &= \mathbb{E}_X \left[ \log{\frac{1}{P(X)}} \right]
    \end{align}
\end{definition}
\begin{remark}
Entropy function $H$ is
    \begin{enumerate}
        \item \textbf{Symmetric}
        \item \textbf{Continuous}
        \item \textbf{Concave}
    \end{enumerate}
\end{remark}
\section{Deriving Entropy Function}
We start with thinking about the desired properties of $H$.
\begin{enumerate}
    \item $H$ is a continuous symmetric function that only depends on the distribution
    \item $H(\frac{1}{2}, \frac{1}{2}) = 1$ (\textbf{Scaling})
    \item $H(\frac{1}{2}, \frac{1}{2}) \le H(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}) \le H(\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}) \ldots$
    \item \textbf{The Grouping Axiom}
    \begin{eg}[Simple Case]
        Let $X \in \{ 0, 1, 2 \}$ and $P_X = (\frac{1}{2}, \frac{1}{4}, \frac{1}{4})$
        We need one bit to tell whether $X = 0$ or $X \in \{ 1, 2 \}$. If $X = 0$, then we only need one bit. If $X \in \{ 1, 2 \}$, we need more bits to determine $X$. Therefore, 
        \[
            H(\frac{1}{2}, \frac{1}{4}, \frac{1}{4}) = H(\frac{1}{2}, \frac{1}{2}) + \frac{1}{2} \cdot 0 + \frac{1}{2} \cdot H(\frac{1}{2}, \frac{1}{2}) = 1.5
        \]
        The first term is the cost of determining whether $X = 0$ or $X \in \{ 1, 2 \}$. The second term is the cost of $X = 0$, which is simply 0. The third term is the cost of identifying $X$ when $X \in \{ 1, 2 \}$
    \end{eg}
    \begin{eg}[Special Case]
        Let $X \in \{ 1, \ldots, 9 \}$, $P_X = (p_1, \ldots, p_9)$, and $q_1 = p_1 + p_2 + p_3, q_2 = p_4 + p_5 + p_6, q_3 = p_7 + p_8 + p_9$. Then,
        \[
            H(p_1, \ldots, p_9) = H(q_1, q_2, q_3) + q_1 \cdot H(\frac{p_1}{q_1}, \frac{p_2}{q_1}, \frac{p_3}{q_1}) + q_2 \cdot H(\frac{p_4}{q_2}, \frac{p_5}{q_2}, \frac{p_6}{q_2}) + H(\frac{p_7}{q_3}, \frac{p_8}{q_3}, \frac{p_9}{q_3})
        \]
    \end{eg}
    \begin{eg}[Another Special Case]
        \[
            H(p_1, p_2, p_3, \ldots, p_m) = H(p_1 + p_2, p_3, \ldots, p_m) + (p_1 + p_2) \cdot H(\frac{p_1}{p_1 + p_2}, \frac{p_2}{p_1 + p_2})
        \]
        This form implies the general case
    \end{eg}
\end{enumerate}

\subsection{Compute $H(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$}
Firstly, using grouping axiom, we know that $H(U_{3^n}) = n \cdot H(U_3)$. Let $k$ be such that $2^k < 3^n < 2^{k+1}$, that is $k = \lfloor n \cdot \log_2 3 \rfloor$ and $k + 1 = \lceil n \cdot \log_2 3 \rceil$. By the third axiom (increasing entropy with more uniformly distributed elements), we have
\begin{align*}
    k = H(U_{2^k}) \le &H(U_{3^n}) \le H(U_{2^{k+1}}) = k + 1
    \\ \frac{\lfloor n \cdot \log_2 3 \rfloor}{n} \le &H(U_3) \le \frac{\lceil n \cdot \log_2 3 \rceil}{n}
\end{align*}
for all $n$. Therefore,
\[H(U_3) = \log_2 3\]

\subsection{Compute $H(p, q)$}
Assume $p = \frac{k}{m}, q = \frac{m-k}{m}$, where $k, m$ are integers. Take $U_m = (\frac{1}{m}, \ldots, \frac{1}{m})$. By partitioning $m = k + (m - k)$, we have
\begin{align*}
    H(U_m) &= H(p, q) + p \cdot H(U_k) + q \cdot H(U_{m-k})
    \\ H(p, q) &= H(U_m) - p \cdot H(U_k) - q \cdot H(U_{m-k})
    \\ &= \log m - p \cdot \log k - q \cdot (\log (m - k))
    \\ &= p \cdot (\log m - \log k) + q \cdot (\log m - \log (m-k))
    \\ &= - p \cdot \log p - q \cdot \log q
\end{align*}

\section{Fundamental Inequalities}
\begin{theorem}
    If $X$ is a random variable over $n$ elements,
    \begin{align*}
        0 \le H(X) \le \log n
    \end{align*}
    The first inequality comes from $\log$ being a concave function trivially. The second inequality comes from Jenson's inequality
\end{theorem}
\begin{note}
    The intuition of this inequality is that any random variable of $n$ elements should require at most the number of bits required to describe a uniform distribution over $n$ elements.
\end{note}
\begin{lemma}[Jensen's Inequality]
    A concave function $f$: for every $t_1, t_2$ (in the interval of the concavity) and every $0 \le \lambda \le 1$,
    \begin{align}
        \lambda \cdot f(t_1) + (1 - \lambda) \cdot f(t_2) \le f(\lambda \cdot t_1 + (1 - \lambda) \cdot t_2)
    \end{align}
    A more general form: for $t_1, \ldots, t_n$ and $\lambda_1, \ldots \lambda_n$
    \begin{align}
        \sum_{i} \lambda_i f(t_i) \le f(\sum_i \lambda_i \cdot t_i)
    \end{align}
    In expectation form, for every random variable $X$,
    \begin{align}
        \mathbb{E}(f(X)) \le f(\mathbb{E}(X))
    \end{align}
\end{lemma}
\begin{proof}[Proof of Second Inequality]
    \begin{align}
        H(X) = \sum_{i}^{n} P(X = x_i) \log \frac{1}{P(X = x_i)} \le \log (\sum_{i}^{n} P(X = x_i) \cdot \frac{1}{P(X = x_i)}) = \log n 
    \end{align}
\end{proof}

\subsection{Conditional Entropy}
Intuitively, for random variable $X, Y$, the conditional entropy $H(X | Y)$ will be the uncertainty remained in $X$ after we observe $Y$.
\begin{align}
    H(X | Y) &= \mathbb{E}_{Y \to b} H(X | Y = b) = \sum_{b} P(Y = b) \cdot H(X | Y = b)
    \\ &= - \sum_{b} P(Y = b) \cdot (\sum_{a} P(X = a | Y = b) \cdot \log P(X = a | Y = b))
    \\ &= - \sum_{a, b} P(X = a, Y = b) \cdot \log P(X = a | Y = b)
    \\ &= - \mathbb{E}_{X, Y} \left[ \log P(X | Y) \right]
\end{align}